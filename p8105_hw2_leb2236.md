P8105 HW2
================
Laura Bulmer
2024-10-02

## Problem 1

First I am going to import and then read/clean the NYC Transit data.

``` r
transit_data = 
  read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", 
           col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) %>% 
  janitor :: clean_names() %>% 
  select(
    line, station_name, station_latitude, station_longitude, 
    starts_with("route"), entry, exit_only, vending, entrance_type, 
    ada) %>% 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
transit_data
```

    ## # A tibble: 1,868 × 20
    ##    line     station_name station_latitude station_longitude route1 route2 route3
    ##    <chr>    <chr>                   <dbl>             <dbl> <chr>  <chr>  <chr> 
    ##  1 4 Avenue 25th St                  40.7             -74.0 R      <NA>   <NA>  
    ##  2 4 Avenue 25th St                  40.7             -74.0 R      <NA>   <NA>  
    ##  3 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ##  4 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ##  5 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ##  6 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ##  7 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ##  8 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ##  9 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ## 10 4 Avenue 53rd St                  40.6             -74.0 R      <NA>   <NA>  
    ## # ℹ 1,858 more rows
    ## # ℹ 13 more variables: route4 <chr>, route5 <chr>, route6 <chr>, route7 <chr>,
    ## #   route8 <chr>, route9 <chr>, route10 <chr>, route11 <chr>, entry <lgl>,
    ## #   exit_only <chr>, vending <chr>, entrance_type <chr>, ada <lgl>

The next bit of code selects the station name and line, and uses
distinct() command to obtain all unique combinations.

``` r
transit_data |> 
  select(station_name, line) |> 
  distinct()
```

    ## # A tibble: 465 × 2
    ##    station_name             line    
    ##    <chr>                    <chr>   
    ##  1 25th St                  4 Avenue
    ##  2 36th St                  4 Avenue
    ##  3 45th St                  4 Avenue
    ##  4 53rd St                  4 Avenue
    ##  5 59th St                  4 Avenue
    ##  6 77th St                  4 Avenue
    ##  7 86th St                  4 Avenue
    ##  8 95th St                  4 Avenue
    ##  9 9th St                   4 Avenue
    ## 10 Atlantic Av-Barclays Ctr 4 Avenue
    ## # ℹ 455 more rows

The next bit of code is similar but filters according to ADA compliance.

``` r
transit_data |> 
  filter(ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```

    ## # A tibble: 84 × 2
    ##    station_name                   line           
    ##    <chr>                          <chr>          
    ##  1 Atlantic Av-Barclays Ctr       4 Avenue       
    ##  2 DeKalb Av                      4 Avenue       
    ##  3 Pacific St                     4 Avenue       
    ##  4 Grand Central                  42nd St Shuttle
    ##  5 34th St                        6 Avenue       
    ##  6 47-50th Sts Rockefeller Center 6 Avenue       
    ##  7 Church Av                      6 Avenue       
    ##  8 21st St                        63rd Street    
    ##  9 Lexington Av                   63rd Street    
    ## 10 Roosevelt Island               63rd Street    
    ## # ℹ 74 more rows

The next bit of code is used to determine the proportion of stations
without vending that allow entrance.

``` r
transit_data |> 
  filter(vending == "NO") |> 
  pull(entry) |> 
  mean()
```

    ## [1] 0.3770492

Lastly, we write a code chunk to identify stations that serve the A
train, and to assess how many of these are ADA compliant. We convert the
data from wide to long format. Then we filter on A train and on ADA
compliance, and use select() and distinct() to obstain dataframes with
the required stations in rows.

``` r
transit_data |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A") |> 
  select(station_name, line) |> 
  distinct()
```

    ## # A tibble: 60 × 2
    ##    station_name                  line           
    ##    <chr>                         <chr>          
    ##  1 Times Square                  42nd St Shuttle
    ##  2 125th St                      8 Avenue       
    ##  3 145th St                      8 Avenue       
    ##  4 14th St                       8 Avenue       
    ##  5 168th St - Washington Heights 8 Avenue       
    ##  6 175th St                      8 Avenue       
    ##  7 181st St                      8 Avenue       
    ##  8 190th St                      8 Avenue       
    ##  9 34th St                       8 Avenue       
    ## 10 42nd St                       8 Avenue       
    ## # ℹ 50 more rows

``` r
transit_data |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A", ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```

    ## # A tibble: 17 × 2
    ##    station_name                  line            
    ##    <chr>                         <chr>           
    ##  1 14th St                       8 Avenue        
    ##  2 168th St - Washington Heights 8 Avenue        
    ##  3 175th St                      8 Avenue        
    ##  4 34th St                       8 Avenue        
    ##  5 42nd St                       8 Avenue        
    ##  6 59th St                       8 Avenue        
    ##  7 Inwood - 207th St             8 Avenue        
    ##  8 West 4th St                   8 Avenue        
    ##  9 World Trade Center            8 Avenue        
    ## 10 Times Square-42nd St          Broadway        
    ## 11 59th St-Columbus Circle       Broadway-7th Ave
    ## 12 Times Square                  Broadway-7th Ave
    ## 13 8th Av                        Canarsie        
    ## 14 Franklin Av                   Franklin        
    ## 15 Euclid Av                     Fulton          
    ## 16 Franklin Av                   Fulton          
    ## 17 Howard Beach                  Rockaway

## Problem \#2

First, we are going to import the Mr. Trash Wheel dataset. Then we read
and clean it.

``` r
mr_trash_wheel = 
  readxl :: read_excel("./data/202409 Trash Wheel Collection Data.xlsx", 
                       sheet = 1, range = "A2:N586") %>% 
  janitor::clean_names() %>% 
  mutate(sports_balls = as.integer(sports_balls)) %>% 
  mutate(trash_wheel = "Mr. Trash Wheel") %>% 
  relocate(trash_wheel)
```

Then we will do the same thing for the Professor Trash Wheel and Gwynnda
datasets.

``` r
prof_trash_wheel = 
  readxl :: read_excel("./data/202409 Trash Wheel Collection Data.xlsx", 
                       sheet = 2, range = "A2:M108") %>% 
  janitor::clean_names() %>% 
  mutate(trash_wheel = "Prof. Trash Wheel") %>% 
  mutate(year =as.character(year)) %>% 
  relocate(trash_wheel)

gwynnda_wheel = 
  readxl :: read_excel("./data/202409 Trash Wheel Collection Data.xlsx", 
                       sheet = 4, range = "A2:L158") %>% 
  janitor::clean_names() %>% 
  mutate(trash_wheel = "Gwynnda Trash Wheel") %>% 
  mutate(year =as.character(year)) %>% 
  relocate(trash_wheel)
```

I’m using the below code to check the number of rows/observations in
each datset. This will help me confirm if I have merged the data sets
correctly in the following step.

``` r
nrow(mr_trash_wheel)
```

    ## [1] 584

``` r
nrow(prof_trash_wheel)
```

    ## [1] 106

``` r
nrow(gwynnda_wheel)
```

    ## [1] 156

Next, we will merge the three datasets into one by binding the rows.

``` r
trash_wheel_df = bind_rows(mr_trash_wheel, prof_trash_wheel,gwynnda_wheel)
```

#### Summary of combined data

The code below is used to determine some statistics used in the summary.

``` r
gwynnda_june =
  gwynnda_wheel %>% 
  filter(
    month == "June",
    year == "2022"
  )

sum(select(gwynnda_june, cigarette_butts))
```

    ## [1] 18120

Our combined dataset has a total of 846 rows and 15 columns. Key
variables include trash wheel name (trash_wheel), dumpster number
(dumpster), weight in tons (weight_tons), and volume in cubic yards
(volume_cubic_yards). Columns are also included for each type of
garbage.

The total weight of trash collected by Professor Trash Wheel for the
available data was 216.26 tons.

The total number of cigarette butts collected by Gwynnda in June of 2022
was 18,120.

## Problem 3

First, we are going to import our three datasets relevate to Great
British Bakeoff. Then we are going to clean and tidy them so we can
eventually merge them.

``` r
bakers_df =
  read.csv("./data/bakers.csv") %>% 
  janitor :: clean_names() %>% 
  relocate(series) %>% 
  separate(baker_name, c('baker', 'baker_last_name'))
```

    ## Warning: Expected 2 pieces. Additional pieces discarded in 10 rows [8, 20, 60, 76, 80,
    ## 90, 96, 102, 108, 110].

``` r
bakes_df =
   read.csv("./data/bakes.csv") %>% 
  janitor :: clean_names()

results_df =
   read.csv("./data/results.csv", skip =2) %>% 
  janitor :: clean_names()
```

The bakes and results datasets have similar columns (series, episode,
baker) so I’ll join those first.

``` r
bakes_results_df = 
  left_join(results_df, bakes_df)
```

    ## Joining with `by = join_by(series, episode, baker)`

Next I am working on joining our combined bakes/results dataset with the
bakers dataset. Within this step I also am reodering the columns to
first indicate the series and episode, then information on the baker,
then the results and what they baked. I then took a look at the data and
it appears that if result is N/A, then the baker did not participate in
that episode. Therefore I am removing rows which indicate N/A for
result.

``` r
gbbs = 
  left_join(bakes_results_df, bakers_df) %>% 
  relocate(series, episode, baker, baker_last_name, baker_age, hometown, baker_occupation, 
           technical, result) %>% 
  drop_na(result) %>% 
  arrange(series, episode, technical)
```

    ## Joining with `by = join_by(series, baker)`

We’ll come back to anti_join before submitting.

``` r
results_bakes_unmatched = anti_join(results_df, bakes_df)
```

    ## Joining with `by = join_by(series, episode, baker)`

Now I am exporting my resulting data frame into a csv format.

``` r
write_csv(gbbs,"./data/gbbs_dataset.csv")
```

As a summary, I first imported each dataset and did basic renaming of
the columns with the clean_name() command. I also pulled the series to
the first column for each, and then separated baker name into first and
last in the bakers dataset to be consistent with the others. I took a
look at the columns in each individual dataset and the data within them.
I then combined the bakes and results dataset so that they were easier
to work with. I then combined with the bakers dataset, and arranged the
columns in a more logical order. I arranged the data to appear in order
of series, then episode, the technical, which I interpreted to mean the
ranked results of the technical portion of the show.

Now I’m creating a table showing the winners of seasons 5-10.

``` r
winners_sbs = 
    gbbs %>% 
    filter(series >=5, result %in% c("STAR BAKER", "WINNER")) %>% 
    select(series, episode, baker, result)
winners_sbs
```

    ##    series episode     baker     result
    ## 1       5       1     Nancy STAR BAKER
    ## 2       5       2   Richard STAR BAKER
    ## 3       5       3      Luis STAR BAKER
    ## 4       5       4   Richard STAR BAKER
    ## 5       5       5      Kate STAR BAKER
    ## 6       5       6    Chetna STAR BAKER
    ## 7       5       7   Richard STAR BAKER
    ## 8       5       8   Richard STAR BAKER
    ## 9       5       9   Richard STAR BAKER
    ## 10      5      10     Nancy     WINNER
    ## 11      6       1     Marie STAR BAKER
    ## 12      6       2       Ian STAR BAKER
    ## 13      6       3       Ian STAR BAKER
    ## 14      6       4       Ian STAR BAKER
    ## 15      6       5    Nadiya STAR BAKER
    ## 16      6       6       Mat STAR BAKER
    ## 17      6       7     Tamal STAR BAKER
    ## 18      6       8    Nadiya STAR BAKER
    ## 19      6       9    Nadiya STAR BAKER
    ## 20      6      10    Nadiya     WINNER
    ## 21      7       1      Jane STAR BAKER
    ## 22      7       2   Candice STAR BAKER
    ## 23      7       3       Tom STAR BAKER
    ## 24      7       4 Benjamina STAR BAKER
    ## 25      7       5   Candice STAR BAKER
    ## 26      7       6       Tom STAR BAKER
    ## 27      7       7    Andrew STAR BAKER
    ## 28      7       8   Candice STAR BAKER
    ## 29      7       9    Andrew STAR BAKER
    ## 30      7      10   Candice     WINNER
    ## 31      8       1    Steven STAR BAKER
    ## 32      8       2    Steven STAR BAKER
    ## 33      8       3     Julia STAR BAKER
    ## 34      8       4      Kate STAR BAKER
    ## 35      8       5    Sophie STAR BAKER
    ## 36      8       6      Liam STAR BAKER
    ## 37      8       7    Steven STAR BAKER
    ## 38      8       8    Stacey STAR BAKER
    ## 39      8       9    Sophie STAR BAKER
    ## 40      8      10    Sophie     WINNER
    ## 41      9       1     Manon STAR BAKER
    ## 42      9       2     Rahul STAR BAKER
    ## 43      9       3     Rahul STAR BAKER
    ## 44      9       4       Dan STAR BAKER
    ## 45      9       5   Kim-Joy STAR BAKER
    ## 46      9       6    Briony STAR BAKER
    ## 47      9       7   Kim-Joy STAR BAKER
    ## 48      9       8      Ruby STAR BAKER
    ## 49      9       9      Ruby STAR BAKER
    ## 50      9      10     Rahul     WINNER
    ## 51     10       1  Michelle STAR BAKER
    ## 52     10       2     Alice STAR BAKER
    ## 53     10       3   Michael STAR BAKER
    ## 54     10       4     Steph STAR BAKER
    ## 55     10       5     Steph STAR BAKER
    ## 56     10       6     Steph STAR BAKER
    ## 57     10       7     Henry STAR BAKER
    ## 58     10       8     Steph STAR BAKER
    ## 59     10       9     Alice STAR BAKER
    ## 60     10      10     David     WINNER

The outcomes in the table largely made sense, however there were a few
surprises. Rahul won season 9 after not having been star baker since
episode 3, and David won season 10 after not having won any star baker
awards.

Now I am going to import the viewers dataset.

``` r
viewers_df = 
  read_csv("./data/viewers.csv") %>% 
  janitor :: clean_names()
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
viewers_df_longer =
  pivot_longer(
    viewers_df,
    series_1:series_10,
    names_to = "series",
    values_to = "viewership"
  ) %>% 
  relocate(series) %>% 
  arrange(series, episode)
```
