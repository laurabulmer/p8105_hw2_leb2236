---
title: "P8105 HW2"
author: "Laura Bulmer"
date: 2024-10-02
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
```

## Problem 1

First I am going to import and then read/clean the NYC Transit data.

```{r}
transit_data = 
  read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", 
           col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) %>% 
  janitor :: clean_names() %>% 
  select(
    line, station_name, station_latitude, station_longitude, 
    starts_with("route"), entry, exit_only, vending, entrance_type, 
    ada) %>% 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
transit_data
```

The next bit of code selects the station name and line, and uses distinct() command to 
obtain all unique combinations.
```{r}
transit_data |> 
  select(station_name, line) |> 
  distinct()
```

The next bit of code is similar but filters according to ADA compliance.
```{r}
transit_data |> 
  filter(ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```


The next bit of code is used to determine the proportion of stations without 
vending that allow entrance.
```{r}
transit_data |> 
  filter(vending == "NO") |> 
  pull(entry) |> 
  mean()
```

Lastly, we write a code chunk to identify stations that serve the A train, and to assess how many of these are ADA compliant. We convert the data from wide to long format. Then we filter on A train and on ADA compliance, and use select() and distinct() to obstain dataframes with the required stations in rows.
```{r}
transit_data |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A") |> 
  select(station_name, line) |> 
  distinct()

transit_data |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A", ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```

## Problem #2

First, we are going to import the Mr. Trash Wheel dataset. Then we read and 
clean it. 


```{r}
mr_trash_wheel = 
  readxl :: read_excel("./data/202409 Trash Wheel Collection Data.xlsx", 
                       sheet = 1, range = "A2:N586") %>% 
  janitor::clean_names() %>% 
  mutate(sports_balls = as.integer(sports_balls)) %>% 
  mutate(trash_wheel = "Mr. Trash Wheel") %>% 
  relocate(trash_wheel)
```

Then we will do the same thing for the Professor Trash Wheel and Gwynnda datasets.

```{r}
prof_trash_wheel = 
  readxl :: read_excel("./data/202409 Trash Wheel Collection Data.xlsx", 
                       sheet = 2, range = "A2:M108") %>% 
  janitor::clean_names() %>% 
  mutate(trash_wheel = "Prof. Trash Wheel") %>% 
  mutate(year =as.character(year)) %>% 
  relocate(trash_wheel)

gwynnda_wheel = 
  readxl :: read_excel("./data/202409 Trash Wheel Collection Data.xlsx", 
                       sheet = 4, range = "A2:L158") %>% 
  janitor::clean_names() %>% 
  mutate(trash_wheel = "Gwynnda Trash Wheel") %>% 
  mutate(year =as.character(year)) %>% 
  relocate(trash_wheel)
```

I'm using the below code to check the number of rows/observations in each datset.
This will help me confirm if I have merged the data sets correctly in the 
following step.

```{r}
nrow(mr_trash_wheel)
nrow(prof_trash_wheel)
nrow(gwynnda_wheel)
```

Next, we will merge the three datasets into one by binding the rows.

```{r}
trash_wheel_df = bind_rows(mr_trash_wheel, prof_trash_wheel,gwynnda_wheel)
```

#### Summary of combined data

The code below is used to determine some statistics used in the summary.

```{r}
gwynnda_june =
  gwynnda_wheel %>% 
  filter(
    month == "June",
    year == "2022"
  )

sum(select(gwynnda_june, cigarette_butts))
```

Our combined dataset has a total of `r nrow(trash_wheel_df)` rows and `r ncol(trash_wheel_df)` columns. Key variables include trash wheel name (trash_wheel), dumpster number (dumpster), weight in tons (weight_tons), and volume in cubic yards (volume_cubic_yards). Columns are also included for each type of garbage. 

The total weight of trash collected by Professor Trash Wheel for the available 
data was `r sum(select(prof_trash_wheel, weight_tons))` tons. 

The total number of cigarette butts collected by Gwynnda in June of 2022 was 18,120.

## Problem 3

First, we are going to import our three datasets relevate to Great British
Bakeoff. Then we are going to clean and tidy them so we can eventually merge them.

```{r}
bakers_df =
  read.csv("./data/bakers.csv") %>% 
  janitor :: clean_names() %>% 
  relocate(series) %>% 
  separate(baker_name, c('baker', 'baker_last_name'))

bakes_df =
   read.csv("./data/bakes.csv") %>% 
  janitor :: clean_names()

results_df =
   read.csv("./data/results.csv", skip =2) %>% 
  janitor :: clean_names()
```

The bakes and results datasets have similar columns (series, episode, baker) so 
I'll join those first.

```{r}
bakes_results_df = 
  left_join(results_df, bakes_df)
```

Next I am working on joining our combined bakes/results dataset with the bakers
dataset. Within this step I also am reodering the columns to first indicate the
series and episode, then information on the baker, then the results and what they baked.
I then took a look at the data and it appears that if result is N/A, then the 
baker did not participate in that episode. Therefore I am removing rows which 
indicate N/A for result.

```{r}
gbbs = 
  left_join(bakes_results_df, bakers_df) %>% 
  relocate(series, episode, baker, baker_last_name, baker_age, hometown, baker_occupation, 
           technical, result) %>% 
  drop_na(result) %>% 
  arrange(series, episode, technical)
```

We'll come back to anti_join before submitting.
```{r}
results_bakes_unmatched = anti_join(results_df, bakes_df)
```

Now I am exporting my resulting data frame into a csv format.

```{r}
write_csv(gbbs,"./data/gbbs_dataset.csv")
```

As a summary, I first imported each dataset and did basic renaming of the columns 
with the clean_name() command. I also pulled the series to the first column for each, 
and then separated baker name into first and last in the bakers dataset to be 
consistent with the others. I took a look at the columns in each individual dataset
and the data within them. I then combined the bakes and results dataset so that 
they were easier to work with. I then combined with the bakers dataset, and arranged the 
columns in a more logical order. I arranged the data to appear in order of series,
then episode, the technical, which I interpreted to mean the ranked results of the
technical portion of the show.

Now I'm creating a table showing the winners of seasons 5-10.

```{r}
winners_sbs = 
    gbbs %>% 
    filter(series >=5, result %in% c("STAR BAKER", "WINNER")) %>% 
    select(series, episode, baker, result)
winners_sbs
```

The outcomes in the table largely made sense, however there were a few surprises.
Rahul won season 9 after not having been star baker since episode 3, and David won season 10 after not having won any star baker awards.

Now I am going to import the viewers dataset, clean and tidy it, and then print
the first 10 observations.

```{r}
viewers_df = 
  read_csv("./data/viewers.csv") %>% 
  janitor :: clean_names()

viewers_df_longer =
  pivot_longer(
    viewers_df,
    series_1:series_10,
    names_to = "series",
    values_to = "viewership"
  ) %>% 
  relocate(series) %>% 
  arrange(series, episode) %>% 
  drop_na(viewership)

head(viewers_df_longer, 10)
```

Now I'm going to pull a few statistics from this dataset.

```{r}
series_1 = 
  viewers_df_longer %>% 
  filter(series == "series_1")

series_5 = 
  viewers_df_longer %>% 
  filter(series == "series_5")
```

The mean viewership in series 1 is `r mean(pull(series_1, viewership), 2)`.
The mean viewership in series 5 is `r mean(pull(series_5, viewership), 2)`.
