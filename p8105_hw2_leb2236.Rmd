---
title: "P8105 HW2"
author: "Laura Bulmer"
date: 2024-10-02
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
```

## Problem 1

First I am going to import and then read/clean the NYC Transit data.

```{r}
transit_data = 
  read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", 
           col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) %>% 
  janitor :: clean_names() %>% 
  select(
    line, station_name, station_latitude, station_longitude, 
    starts_with("route"), entry, exit_only, vending, entrance_type, 
    ada) %>% 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
transit_data
```

The next bit of code selects the station name and line, and uses distinct() command to 
obtain all unique combinations.
```{r}
transit_data |> 
  select(station_name, line) |> 
  distinct()
```

The next bit of code is similar but filters according to ADA compliance.
```{r}
transit_data |> 
  filter(ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```


The next bit of code is used to determine the proportion of stations without 
vending that allow entrance.
```{r}
transit_data |> 
  filter(vending == "NO") |> 
  pull(entry) |> 
  mean()
```

Lastly, we write a code chunk to identify stations that serve the A train, and to assess how many of these are ADA compliant. We convert the data from wide to long format. Then we filter on A train and on ADA compliance, and use select() and distinct() to obstain dataframes with the required stations in rows.
```{r}
transit_data |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A") |> 
  select(station_name, line) |> 
  distinct()

transit_data |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A", ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```

## Problem #2

First, we are going to import the Mr. Trash Wheel dataset. Then we read and 
clean it. 


```{r}
mr_trash_wheel = 
  readxl :: read_excel("./data/202409 Trash Wheel Collection Data.xlsx", 
                       sheet = 1, range = "A2:N586") %>% 
  janitor::clean_names() %>% 
  mutate(sports_balls = as.integer(sports_balls)) %>% 
  mutate(trash_wheel = "Mr. Trash Wheel") %>% 
  relocate(trash_wheel)
```

Then we will do the same thing for the Professor Trash Wheel and Gwynnda datasets.

```{r}
prof_trash_wheel = 
  readxl :: read_excel("./data/202409 Trash Wheel Collection Data.xlsx", 
                       sheet = 2, range = "A2:M108") %>% 
  janitor::clean_names() %>% 
  mutate(trash_wheel = "Prof. Trash Wheel") %>% 
  mutate(year =as.character(year)) %>% 
  relocate(trash_wheel)

gwynnda_wheel = 
  readxl :: read_excel("./data/202409 Trash Wheel Collection Data.xlsx", 
                       sheet = 4, range = "A2:L158") %>% 
  janitor::clean_names() %>% 
  mutate(trash_wheel = "Gwynnda Trash Wheel") %>% 
  mutate(year =as.character(year)) %>% 
  relocate(trash_wheel)
```

I'm using the below code to check the number of rows/observations in each datset.
This will help me confirm if I have merged the data sets correctly in the 
following step.

```{r}
nrow(mr_trash_wheel)
nrow(prof_trash_wheel)
nrow(gwynnda_wheel)
```

Next, we will merge the three datasets into one by binding the rows.

```{r}
trash_wheel_df = bind_rows(mr_trash_wheel, prof_trash_wheel,gwynnda_wheel)
```

#### Summary of combined data

The code below is used to determine some statistics used in the summary.

```{r}
gwynnda_june =
  gwynnda_wheel %>% 
  filter(
    month == "June",
    year == "2022"
  )

sum(select(gwynnda_june, cigarette_butts))
```

Our combined dataset has a total of `r nrow(trash_wheel_df)` rows and `r ncol(trash_wheel_df)` columns. Key variables include trash wheel name (trash_wheel), dumpster number (dumpster), weight in tons (weight_tons), and volume in cubic yards (volume_cubic_yards). Columns are also included for each type of garbage. 

The total weight of trash collected by Professor Trash Wheel for the available 
data was `r sum(select(prof_trash_wheel, weight_tons))` tons. 

The total number of cigarette butts collected by Gwynnda in June of 2022 was 18,120.

## Problem 3

First, we are going to import our three datasets relevate to Great British
Bakeoff. Then we are going to clean and tidy them so we can eventually merge them.

```{r}
bakers_df =
  read.csv("./data/bakers.csv") %>% 
  janitor :: clean_names() %>% 
  relocate(series) %>% 
  separate(baker_name, c('baker', 'baker_last_name'))


bakes_df =
   read.csv("./data/bakes.csv") %>% 
  janitor :: clean_names()

results_df =
   read.csv("./data/results.csv", skip =2) %>% 
  janitor :: clean_names()
```

The bakes and results datasets have similar columns (series, episode, baker) so 
I'll join those first.

```{r}
bakes_results_df = 
  left_join(results_df, bakes_df)
```

Next I am working on joining our combined bakes/results dataset with the bakers
dataset. Within this step I also am reodering the columns to first indicate the
series and episode, then information on the baker, then the results and what they baked.

```{r}
gbbs = 
  left_join(bakes_results_df, bakers_df) %>% 
  relocate(series, episode, baker, baker_last_name, baker_age, hometown, baker_occupation, 
           technical, result)
```

